{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4376ee1f",
   "metadata": {},
   "source": [
    "# Unit Testing Basics\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "Unit tests are a way to “lock in” the behavior of your code. After this lesson, you should be able to:\n",
    "\n",
    "- Use `assert` as the core mechanism behind tests\n",
    "- Write tests with `pytest` (recommended in most projects)\n",
    "- Test both expected behavior and failure behavior (exceptions)\n",
    "- Reuse test data with fixtures\n",
    "- Test file output safely without touching your real filesystem\n",
    "- Understand why mocking / dependency injection matters for unit tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46c7b6",
   "metadata": {},
   "source": [
    "## What is a unit test and why do we care?\n",
    "\n",
    "A **unit test** is a small automated check for a small unit of code (often a single function). The goal is simple:\n",
    "given some input, verify the output or behavior is what we expect.\n",
    "\n",
    "In real projects, tests matter because code changes constantly: new features, bug fixes, refactors, performance work.\n",
    "Tests give you a safety net. If a change breaks something old, a test should fail quickly and tell you exactly what went wrong.\n",
    "\n",
    "- Unit tests should be **fast**, **repeatable**, and **deterministic**.\n",
    "- A unit test checks a **single behavior** (or a very small set of related behaviors).\n",
    "- Good tests act as **documentation**: they show what the code is supposed to do.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9781e4c",
   "metadata": {},
   "source": [
    "## Assertions (The core idea on what testing frameworks build on)\n",
    "\n",
    "At the heart of unit testing is the concept of an **assertion**: a statement that must be true.\n",
    "In Python, `assert condition` raises an `AssertionError` if the condition is false.\n",
    "\n",
    "Most testing frameworks (like `pytest`) are structured ways to run many `assert`s and report failures clearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b1109",
   "metadata": {},
   "source": [
    "### Example function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e0c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_email(email: str) -> str:\n",
    "    return email.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da96763f",
   "metadata": {},
   "source": [
    "### “Test” using plain assert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d2e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normalize_email_basic():\n",
    "    result = normalize_email(\"  Carlos@Example.COM \")\n",
    "    assert result == \"carlos@example.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06911aac",
   "metadata": {},
   "source": [
    "## Arrange–Act–Assert\n",
    "\n",
    "When tests get longer, structure matters. A common pattern is **Arrange–Act–Assert (AAA)**:\n",
    "\n",
    "- **Arrange**: prepare inputs and context  \n",
    "- **Act**: call the function  \n",
    "- **Assert**: verify the result  \n",
    "\n",
    "This pattern makes tests easier to read later, especially when a test fails.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a95b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tax(price: float, tax_rate: float) -> float:\n",
    "    return round(price * (1 + tax_rate), 2)\n",
    "\n",
    "def test_add_tax():\n",
    "    # Arrange\n",
    "    price = 100.0\n",
    "    tax_rate = 0.16\n",
    "\n",
    "    # Act\n",
    "    total = add_tax(price, tax_rate)\n",
    "\n",
    "    # Assert\n",
    "    assert total == 116.00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e2236",
   "metadata": {},
   "source": [
    "## pytest\n",
    "### Usually the best starting point\n",
    "\n",
    "In modern Python teams, `pytest` is commonly preferred because it keeps tests readable:\n",
    "\n",
    "- You write tests as normal functions\n",
    "- You use plain `assert`\n",
    "- It gives good failure messages (diffs, values, traces)\n",
    "- It supports fixtures for reusable setup\n",
    "\n",
    "### Typical structure:\n",
    "\n",
    "```\n",
    "project/\n",
    "  src/\n",
    "    pricing.py\n",
    "  tests/\n",
    "    test_pricing.py\n",
    "```\n",
    "\n",
    "### How to run:\n",
    "\n",
    "- `pytest -q`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef02ad",
   "metadata": {},
   "source": [
    "### Example code: business rule + validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff278a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_discount(price: float, discount: float) -> float:\n",
    "    \"\"\"\n",
    "    discount in range [0, 1]\n",
    "    \"\"\"\n",
    "    if not 0 <= discount <= 1:\n",
    "        raise ValueError(\"discount must be between 0 and 1\")\n",
    "    return round(price * (1 - discount), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b78db",
   "metadata": {},
   "source": [
    "### Tests (pytest style)\n",
    "\n",
    "> Note: In a real project, this code would live in `tests/test_pricing.py` and you would run it with `pytest`.\n",
    "> Here I show the standard pytest syntax.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d255b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest example (place each of this examples inside tests/test_pricing.py)\n",
    "import pytest\n",
    "\n",
    "def test_apply_discount_happy_path():\n",
    "    assert apply_discount(100, 0.2) == 80.0\n",
    "\n",
    "def test_apply_discount_zero_discount():\n",
    "    assert apply_discount(100, 0.0) == 100.0\n",
    "\n",
    "def test_apply_discount_invalid_discount_raises():\n",
    "    with pytest.raises(ValueError):\n",
    "        apply_discount(100, 1.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab11c9",
   "metadata": {},
   "source": [
    "## Happy path vs edge cases\n",
    "\n",
    "A common beginner mistake is writing only one test for a function and assuming it’s enough.\n",
    "Real bugs often live in **edge cases**.\n",
    "\n",
    "From a data engineering perspective, edge cases often include:\n",
    "- empty strings\n",
    "- missing values\n",
    "- malformed numeric formats\n",
    "- unexpected whitespace\n",
    "- negative numbers or invalid ranges\n",
    "\n",
    "Below is a small example where we decide (explicitly) what happens when `b == 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_divide(a: float, b: float) -> float:\n",
    "    if b == 0:\n",
    "        raise ZeroDivisionError(\"b must not be zero\")\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f31640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest example tests\n",
    "import pytest\n",
    "\n",
    "def test_safe_divide_normal():\n",
    "    assert safe_divide(10, 2) == 5\n",
    "\n",
    "def test_safe_divide_raises_on_zero():\n",
    "    with pytest.raises(ZeroDivisionError):\n",
    "        safe_divide(10, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88865bf",
   "metadata": {},
   "source": [
    "## Fixtures\n",
    "\n",
    "A **fixture** is reusable test setup. Instead of repeating the same data in every test,\n",
    "you define it once and inject it into any test that needs it.\n",
    "\n",
    "Tip: This is especially helpful in ETL contexts where “sample records” show up everywhere.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest fixture example\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def sample_orders():\n",
    "    return [\n",
    "        {\"id\": 1, \"amount\": 100.0},\n",
    "        {\"id\": 2, \"amount\": 250.5},\n",
    "        {\"id\": 3, \"amount\": 0.0},\n",
    "    ]\n",
    "\n",
    "def total_amount(orders):\n",
    "    return sum(o[\"amount\"] for o in orders)\n",
    "\n",
    "def test_total_amount(sample_orders):\n",
    "    assert total_amount(sample_orders) == 350.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6efca2",
   "metadata": {},
   "source": [
    "## Testing file output safely\n",
    "\n",
    "ETL code often reads/writes files. A bad testing practice is writing to real paths like `./output/report.txt`\n",
    "because tests can:\n",
    "\n",
    "- overwrite real files\n",
    "- conflict with other runs\n",
    "- behave differently on different machines\n",
    "\n",
    "Pytest provides `tmp_path`, a temporary directory unique to the test run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef082f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def write_report(path: Path, content: str) -> None:\n",
    "    path.write_text(content, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f08def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest example using tmp_path\n",
    "def test_write_report(tmp_path):\n",
    "    report_path = tmp_path / \"report.txt\"\n",
    "    write_report(report_path, \"hello\")\n",
    "\n",
    "    assert report_path.exists()\n",
    "    assert report_path.read_text(encoding=\"utf-8\") == \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bef753",
   "metadata": {},
   "source": [
    "## Mocking and dependency injection (unit tests should not call the real world)\n",
    "\n",
    "Unit tests should be fast and stable. External systems (network calls, DB queries, cloud storage)\n",
    "are slow and can fail for reasons unrelated to your code.\n",
    "\n",
    "A clean approach is **dependency injection**: pass the dependency in. Then in tests, you pass a fake version.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61175b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_user(user: dict) -> dict:\n",
    "    return {\"id\": user[\"id\"], \"email\": user[\"email\"].strip().lower()}\n",
    "\n",
    "def process_users(fetch_users_func):\n",
    "    users = fetch_users_func()\n",
    "    return [transform_user(u) for u in users]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c044349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process_users_with_fake_fetch():\n",
    "    def fake_fetch():\n",
    "        return [{\"id\": 1, \"email\": \"  A@B.COM \"}]\n",
    "\n",
    "    result = process_users(fake_fetch)\n",
    "    assert result == [{\"id\": 1, \"email\": \"a@b.com\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff0dbab",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "These exercises are designed to practice:\n",
    "- normal expected behavior\n",
    "- exception testing\n",
    "- file round-trip testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7304973",
   "metadata": {},
   "source": [
    "## Exercise 1: Test input cleaning + exception behavior\n",
    "\n",
    "### Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64311a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_country(country: str) -> str:\n",
    "    country = country.strip().title()\n",
    "    if not country:\n",
    "        raise ValueError(\"country must not be empty\")\n",
    "    return country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c89637",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Write 3 tests:\n",
    "\n",
    "- `\"  mexico \"` → `\"Mexico\"`\n",
    "- `\"Canada\"` → `\"Canada\"`\n",
    "- `\"\"` raises `ValueError`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5396681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Have a little help, fill in the required code\n",
    "def test_clean_country_normal():\n",
    "    pass\n",
    "\n",
    "def test_clean_country_already_clean():\n",
    "    pass\n",
    "\n",
    "def test_clean_country_empty_raises():\n",
    "    with pytest.raises(ValueError):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d2f2f",
   "metadata": {},
   "source": [
    "## Exercise 2: Test a mini ETL parsing function\n",
    "\n",
    "### Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4354bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_amount(value: str) -> float:\n",
    "    value = value.replace(\",\", \"\").strip()\n",
    "    return float(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c5d150",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "- `\"1,234.50\"` → `1234.50`\n",
    "- `\" 0 \"` → `0.0`\n",
    "- `\"abc\"` raises `ValueError`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee242f31",
   "metadata": {},
   "source": [
    "## Exercise 3: File round-trip test with tmp_path\n",
    "\n",
    "### Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c068a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def write_json(path: Path, data: dict) -> None: # Note it receives a Path object, not a path string\n",
    "    path.write_text(json.dumps(data), encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e7e6db",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "Use `tmp_path` to verify:\n",
    "\n",
    "- file exists  \n",
    "- content loads back to the same dict  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee72a9",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cc81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Fill in the spaces\n",
    "\n",
    "def test_write_json_roundtrip(tmp_path):\n",
    "    path = ___ / \"data.json\" # Needs to be a Path object! \n",
    "    payload = {\"a\": 1, \"b\": \"x\"}\n",
    "\n",
    "    write_json(path, payload)\n",
    "\n",
    "    assert ___.exists()\n",
    "    loaded = json.loads(___) # Ptss: ___.read_text(encoding=\"utf-8\")\n",
    "    assert ___ # Compare what you loaded with the default payload\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadf5e3d",
   "metadata": {},
   "source": [
    "## Common mistakes (and how to talk about them)\n",
    "\n",
    "A lot of testing pain comes from a few repeating mistakes:\n",
    "\n",
    "- Writing tests that depend on the network / DB / real APIs\n",
    "- Using random test data without controlling it (seed it, or use fixed fixtures) # for seeds check ´random´ module or Numpy's random.seed()\n",
    "- Testing too many things in one test (hard to diagnose failures)\n",
    "- Tests that rely on execution order (tests should be independent)\n",
    "- Not testing failure behavior (exceptions, invalid input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386cc15e",
   "metadata": {},
   "source": [
    "> Content created by [**Carlos Cruz-Maldonado**](https://www.linkedin.com/in/carloscruzmaldonado/).  \n",
    "> I am available to answer any questions or provide further assistance.   \n",
    "> Feel free to reach out to me at any time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
